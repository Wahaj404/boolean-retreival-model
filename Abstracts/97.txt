A Semi-supervised Clustering Approach for Semantic Slot Labelling

natural language processing, semantic slot labelling, semi-supervised learning, interactive systems

Work on training semantic slot labellers for use in Natural Language Processing applications has typically either relied on large amounts of labelled input data, or has assumed entirely unlabelled inputs. The former technique tends to be costly to apply, while the latter is often not as accurate as its supervised counterpart. Here, we present a semi-supervised learning approach that automatically labels the semantic slots in a set of training data and aims to strike a balance between the dependence on labelled data and prediction accuracy. The essence of our algorithm is to cluster clauses based on a similarity function that combines lexical and semantic information. We present experiments that compare different similarity functions for both our semi-supervised setting and a fully unsupervised baseline. While semi-supervised learning expectedly outperforms unsupervised learning, our results show that this effect can be observed based on very few training data instances and that increasing the size of the training data does not lead to better performance, and that lexical and semantic information contribute differently in different domains so that clustering based on both types of information offers the best generalisation.